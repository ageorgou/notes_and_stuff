{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 22: Data Pipelines\n",
    "\n",
    "Applying machine learning algorithms and approaches to real-world data\n",
    "involves a number of practical challenges. In this series of lectures,\n",
    "we will look at tools and ways of working that address questions like:\n",
    "\n",
    "- How can I store and access large amounts of data remotely?\n",
    "- How can I keep track of different versions of datasets?\n",
    "- How can I share my results and make my analyses reproducible by others?\n",
    "\n",
    "Before we get into individual questions, let's take a look at the broader picture of how we handle data, before and after running any analysis or inference methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "More often than not, the data of interest will not be in a directly useable form, or perhaps not even immediately available to you.\n",
    "\n",
    "Before we can train any model, we first need to make sure that the data is available and properly formatted. This can involve a number of steps:\n",
    "\n",
    "- Accessing the data\n",
    "- Cleaning and other preprocessing\n",
    "- Reformatting\n",
    "\n",
    "After this, the data is ready to be used in our algorithm of choice.\n",
    "\n",
    "Image: access -> preprocess -> (formatting ->?) modelling/prediction -> publishing\n",
    "\n",
    "This sequence of steps is sometimes called a **data pipeline**. Another related term used to describe similar workflows is ETL: Extract-Transform-Load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing data\n",
    "\n",
    "If we haven't collected the data ourselves, we will first need to get our (virtual) hands on it! This can be done in a number of ways. For example:\n",
    "- A colleague gives us a file\n",
    "- We connect to a web service that produces the data\n",
    "- We \"scrape\" a web page or other source to extract the data ourselves\n",
    "- We query a database for the particular data we want\n",
    "\n",
    "Sometimes we will need to combine more than one source to get the full set of data that we require.\n",
    "\n",
    "We will talk more about databases in the next lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common element in the above examples is that the data can exist in some **remote** location. How we get it on our own computer will depend on the source, format and size of the data. However, this can often be done programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Downloading data from S3\n",
    "\n",
    "S3 is a storage service offered by Amazon Web Services (AWS). Users can upload datasets which can then be accessed by others. For this example we will look at an open dataset that can be downloaded by anyone.\n",
    "\n",
    "(To run this locally, you will need to install the Python package `boto3`, which will let you communicate with AWS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-003c08417fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "It is not always enough to get hold of the data you want to work with. Sometimes this raw or preliminary data has to be transformed. There are many reasons why:\n",
    "\n",
    "- Errors in data\n",
    "- Dimensionality or size of dataset is too high\n",
    "- We want to focus only on a subset of interest\n",
    "- Raw data does not directly contain variables of interest \n",
    "- Some algorithms are negatively impacted by e.g. imbalances in class frequencies or extreme values\n",
    "\n",
    "Preprocessing steps can include:\n",
    "- Replacing values that are incorrect or cause problems\n",
    "- Filtering, subsampling (discarding samples) or supersampling (repeating samples)\n",
    "- Creating new variables such as by combining existing ones\n",
    "- Removing outliers\n",
    "\n",
    "Aspects of this are often referred to as **\"cleaning\"** the data. This is an important and often undervalued step in the pipeline. These transformations can be performed manually, although tools like [OpenRefine](https://openrefine.org/) can simplify and automate the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting\n",
    "**Maybe should be part of preprocessing?**\n",
    "\n",
    "The code you have written may expect to read in data in a particular format, such as a CSV file, or a collection of files. The result of your preprocessing must therefore be put into the same format.\n",
    "\n",
    "This can often be done through a library. For instance, `pandas` offers several methods for writing out a data frame to a number of commonly-used formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of pandas or json or...?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publishing\n",
    "Make your results and models available:\n",
    "- Serializing results, making things reproducible\n",
    "- Uploading remotely\n",
    "- Metadata and explanation of provenance/meaning\n",
    "- FAIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Data needs to go through a number of steps before it can used in a model.\n",
    "- Doing this process programmatically, not manually, leaves a record and facilitates repetition and verification.\n",
    "- Remote access to data is becoming increasingly important as size grows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
